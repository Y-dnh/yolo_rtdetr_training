"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è/–≤–∞–ª—ñ–¥–∞—Ü—ñ—ó YOLO / RT-DETR –º–æ–¥–µ–ª—ñ –¥–µ—Ç–µ–∫—Ü—ñ—ó –Ω–∞ IR –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è—Ö.
–£—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –∑–Ω–∞—Ö–æ–¥—è—Ç—å—Å—è –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ñ–∞–π–ª—É.

–ü–µ—Ä–µ–º–∏–∫–∞—á MODEL_TYPE –¥–æ–∑–≤–æ–ª—è—î –æ–±—Ä–∞—Ç–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É:
  - "yolo"   -> ultralytics.YOLO
  - "rtdetr"  -> ultralytics.RTDETR
"""

import os
import json
from pathlib import Path
from datetime import datetime
import torch
from ultralytics import YOLO, RTDETR

try:
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt
    import numpy as np
    _HAS_MATPLOTLIB = True
except ImportError:
    _HAS_MATPLOTLIB = False


# =============================================================================
# –í–ò–ë–Ü–† –ê–†–•–Ü–¢–ï–ö–¢–£–†–ò: "yolo" –∞–±–æ "rtdetr"
# =============================================================================
VALID_MODEL_TYPES = {"yolo", "rtdetr"}
MODEL_TYPE = "yolo"        # <-- –ü–ï–†–ï–ú–ò–ö–ê–ß: "yolo" –∞–±–æ "rtdetr"

# =============================================================================
# –ü–ê–†–ê–ú–ï–¢–†–ò, –°–ü–ï–¶–ò–§–Ü–ß–ù–Ü –î–õ–Ø –ö–û–ñ–ù–û–á –ê–†–•–Ü–¢–ï–ö–¢–£–†–ò (–≤–∞–ª—ñ–¥–∞—Ü—ñ—è)
# =============================================================================

# –ö–ª—é—á—ñ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó, —è–∫—ñ —î –¢–Ü–õ–¨–ö–ò —É YOLO
YOLO_ONLY_VAL_KEYS = {
    "agnostic_nms",     # Class-agnostic NMS ‚Äî RT-DETR –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î NMS
    "dnn",              # OpenCV DNN backend ‚Äî —Ç—ñ–ª—å–∫–∏ –¥–ª—è YOLO
}

# –ö–ª—é—á—ñ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó, —è–∫—ñ —î –¢–Ü–õ–¨–ö–ò —É RT-DETR
RTDETR_ONLY_VAL_KEYS: set[str] = set()

# =============================================================================
# –ë–ê–ó–û–í–ê –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø
# =============================================================================
PROJECT_NAME = "yolo26s_yaml"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = os.path.join(BASE_DIR, PROJECT_NAME)
# DATASET_ROOT = os.path.join(BASE_DIR, "dataset_split")
DATASET_ROOT = "D:/dataset_for_training"
YAML_PATH = os.path.join(DATASET_ROOT, "data.yaml")
EXPERIMENT_NAME = "validation_26s_yaml"

TRAINED_MODEL_PATH = os.path.join(PROJECT_DIR, "baseline", "weights", "best.pt")

# –ö–ª–∞—Å–∏ –¥–∞—Ç–∞—Å–µ—Ç—É
CLASSES = {
    0: "person",
    1: "car",
    2: "truck",
}


# =============================================================================
# –ü–ê–†–ê–ú–ï–¢–†–ò –í–ê–õ–Ü–î–ê–¶–Ü–á (–ø–µ—Ä–µ–¥–∞—é—Ç—å—Å—è —è–∫ **kwargs –¥–æ model.val())
# =============================================================================
VALIDATION_CONFIG = {
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–∞—Ç–∞—Å–µ—Ç—É
    "data": YAML_PATH,
    "split": "test",
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–µ—Ç–µ–∫—Ü—ñ—ó
    "conf": 0.25,
    "iou": 0.5,
    "imgsz": 1024,
    "device": None,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è
    "batch": 8,  # –ó–º–µ–Ω—à–µ–Ω–æ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
    "max_det": 300,
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –æ–±—Ä–æ–±–∫–∏
    "rect": True,
    "half": True,
    "augment": False,
    "agnostic_nms": False,   # [YOLO-only] RT-DETR –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î NMS
    "classes": None,
    "single_cls": False,
    "dnn": False,
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤–∏–≤–æ–¥—É
    "save_json": True,
    "save_txt": False,
    "save_conf": True,
    "plots": True,
    "verbose": False,
    "workers": 8,  # 0 —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ multiprocessing —Ç–∞ –ø—Ä–æ–±–ª–µ–º –∑ –ø–∞–º'—è—Ç—Ç—é
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    "visualize": True,
    
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–æ–µ–∫—Ç—É
    "project": PROJECT_DIR,
    "name": EXPERIMENT_NAME,
}


def validate_model_type() -> None:
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —â–æ MODEL_TYPE –º–∞—î –¥–æ–ø—É—Å—Ç–∏–º–µ –∑–Ω–∞—á–µ–Ω–Ω—è."""
    if MODEL_TYPE not in VALID_MODEL_TYPES:
        raise ValueError(
            f"–ù–µ–≤—ñ–¥–æ–º–∏–π MODEL_TYPE: '{MODEL_TYPE}'. "
            f"–î–æ–ø—É—Å—Ç–∏–º—ñ –∑–Ω–∞—á–µ–Ω–Ω—è: {sorted(VALID_MODEL_TYPES)}"
        )


def load_model(model_path: str):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ MODEL_TYPE.
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î —Ç–∏–ø, —è–∫—â–æ –≤ —à–ª—è—Ö—É —î 'rtdetr'.
    
    Args:
        model_path: –®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ
    
    Returns:
        –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å (YOLO –∞–±–æ RTDETR)
    
    Raises:
        ValueError: –Ø–∫—â–æ MODEL_TYPE –Ω–µ–≤—ñ–¥–æ–º–∏–π
    """
    validate_model_type()

    if MODEL_TYPE == "rtdetr" or "rtdetr" in model_path.lower():
        print(f"[Model] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è RT-DETR: {model_path}")
        return RTDETR(model_path)
    else:
        print(f"[Model] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è YOLO: {model_path}")
        return YOLO(model_path)


def filter_config(config: dict, excluded_keys: set) -> dict:
    """
    –§—ñ–ª—å—Ç—Ä—É—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é: –≤–∏–¥–∞–ª—è—î –∫–ª—é—á—ñ, –Ω–µ—Å—É–º—ñ—Å–Ω—ñ –∑ –ø–æ—Ç–æ—á–Ω–æ—é –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–æ—é.
    
    Args:
        config: –í—Ö—ñ–¥–Ω–∏–π —Å–ª–æ–≤–Ω–∏–∫ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
        excluded_keys: –ú–Ω–æ–∂–∏–Ω–∞ –∫–ª—é—á—ñ–≤, —è–∫—ñ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–¥–∞–ª–∏—Ç–∏
    
    Returns:
        dict: –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∏–π —Å–ª–æ–≤–Ω–∏–∫
    """
    removed = set(config.keys()) & excluded_keys
    if removed:
        print(f"[Config] MODEL_TYPE='{MODEL_TYPE}' -> –≤–∏–¥–∞–ª–µ–Ω–æ –Ω–µ—Å—É–º—ñ—Å–Ω—ñ –∫–ª—é—á—ñ: {sorted(removed)}")

    return {k: v for k, v in config.items() if k not in excluded_keys}


def get_val_config(**kwargs) -> dict:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∏–π validation config –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ MODEL_TYPE.
    
    Args:
        **kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä–∏, —â–æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—É—é—Ç—å VALIDATION_CONFIG
    
    Returns:
        dict: –ì–æ—Ç–æ–≤–∏–π –∫–æ–Ω—Ñ—ñ–≥ –¥–ª—è model.val()
    """
    config = {**VALIDATION_CONFIG, **kwargs}

    if MODEL_TYPE == "rtdetr":
        return filter_config(config, YOLO_ONLY_VAL_KEYS)
    elif MODEL_TYPE == "yolo":
        return filter_config(config, RTDETR_ONLY_VAL_KEYS)
    return config


def print_header(model_path: str, config: dict, device: str) -> None:
    """–í–∏–≤–µ–¥–µ–Ω–Ω—è –∑–∞–≥–æ–ª–æ–≤–∫—É –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó."""
    model_name = Path(model_path).name
    print()
    print("=" * 70)
    print("YOLO VALIDATION")
    print("=" * 70)
    print(f"Model: {model_name}")
    print(f"Dataset: {config['data']}")
    print(f"Split: {config['split']}")
    print(f"Image Size: {config['imgsz']}")
    print(f"Conf threshold: {config['conf']}")
    print(f"IoU threshold: {config['iou']}")
    print(f"Max detections: {config['max_det']}")
    print(f"Half (FP16): {config['half']}")
    print(f"Device: {device}")
    print("=" * 70)
    print()


def setup_device() -> str:
    """–í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –ø—Ä–∏—Å—Ç—Ä–æ—é."""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    return device


def validate_model(
    model_path: str = TRAINED_MODEL_PATH,
    **kwargs
) -> object:
    """
    –í–∞–ª—ñ–¥–∞—Ü—ñ—è YOLO –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ.
    
    Args:
        model_path: –®–ª—è—Ö –¥–æ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
        **kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—É—é—Ç—å VALIDATION_CONFIG)
    
    Returns:
        object: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
    """
    # –û—Ç—Ä–∏–º—É—î–º–æ –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ñ—ñ–≥ –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ MODEL_TYPE
    config = get_val_config(**kwargs)
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è device —è–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ
    if config["device"] is None:
        config["device"] = setup_device()
    
    # –í–∏–≤–æ–¥–∏–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
    print_header(model_path, config, config["device"])
    
    print(f"[Validator] Loading {MODEL_TYPE.upper()} model from {Path(model_path).name}...")
    model = load_model(model_path)
    print(f"[Validator] Model loaded successfully!")
    
    # –ó–∞–ø—É—Å–∫ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
    results = model.val(**config)
    
    return results


def _safe_float(x, default=0.0):
    """–ü–æ–≤–µ—Ä—Ç–∞—î float –∞–±–æ default —è–∫—â–æ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–µ."""
    if x is None:
        return default
    try:
        return float(x)
    except (TypeError, ValueError):
        return default


# –ü–æ—Ä–æ–≥–∏ COCO/YOLO –¥–ª—è —Ä–æ–∑–º—ñ—Ä—É –æ–±'—î–∫—Ç—ñ–≤ (–ø–ª–æ—â–∞ –≤ –ø—ñ–∫—Å–µ–ª—è—Ö¬≤)
COCO_AREA_SMALL = 32 ** 2   # area < 32¬≤
COCO_AREA_MEDIUM_MAX = 96 ** 2  # 32¬≤‚Äì96¬≤ = medium, > 96¬≤ = large


# GT —É YOLO ‚Äî —É .txt; pycocotools –ø–æ—Ç—Ä–µ–±—É—î GT —É COCO JSON ‚Üí –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ val —É –æ–¥–∏–Ω —Ñ–∞–π–ª.
def _image_size_fast(img_path: str) -> tuple[int, int] | None:
    """–ü–æ–≤–µ—Ä—Ç–∞—î (width, height) –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –±–µ–∑ –ø–æ–≤–Ω–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è."""
    try:
        from PIL import Image
        with Image.open(img_path) as im:
            return im.size
    except Exception:
        return None


def yolo_split_to_coco_annotations(
    data_yaml_path: str,
    output_json_path: str,
    split: str = "val",
    imgsz: int = None,
) -> str | None:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç—É—î val-—Å–ø–ª—ñ—Ç —É —Ñ–æ—Ä–º–∞—Ç—ñ YOLO (–ª–µ–π–±–ª–∏ .txt) —É –æ–¥–∏–Ω COCO JSON.
    width/height —ñ bbox ‚Äî —É –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (—è–∫ —É predictions –ø—Ä–∏ rect=True).
    """
    imgsz = imgsz or VALIDATION_CONFIG.get("imgsz", 640)
    img_paths, dataset_path = _load_yolo_val_data(data_yaml_path, split)
    if not img_paths or not dataset_path:
        return None
    images = []
    annotations = []
    ann_id = 0
    for img_id, img_path in enumerate(img_paths):
        if not os.path.isfile(img_path):
            continue
        wh = _image_size_fast(img_path)
        if wh is None:
            wh = (imgsz, imgsz)
        w_img, h_img = wh
        images.append({
            "id": img_id,
            "file_name": os.path.basename(img_path),
            "width": w_img,
            "height": h_img,
        })
        label_path = _yolo_label_path(img_path, dataset_path)
        if not os.path.isfile(label_path):
            continue
        with open(label_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
        for line in lines:
            parts = line.strip().split()
            if len(parts) < 5:
                continue
            try:
                cls = int(float(parts[0]))
                xc, yc, w_n, h_n = float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])
            except (ValueError, IndexError):
                continue
            x = (xc - w_n / 2) * w_img
            y = (yc - h_n / 2) * h_img
            w = w_n * w_img
            h = h_n * h_img
            area = w * h
            ann_id += 1
            annotations.append({
                "id": ann_id,
                "image_id": img_id,
                "category_id": cls + 1,
                "bbox": [round(x, 2), round(y, 2), round(w, 2), round(h, 2)],
                "area": round(area, 2),
                "iscrowd": 0,
            })
    categories = [{"id": i + 1, "name": name} for i, name in CLASSES.items()]
    coco = {"images": images, "annotations": annotations, "categories": categories}
    try:
        os.makedirs(os.path.dirname(output_json_path) or ".", exist_ok=True)
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(coco, f, indent=2, ensure_ascii=False)
        return output_json_path
    except Exception:
        return None


def _ensure_predictions_coco_json(results, save_dir: str, img_paths: list, imgsz: int) -> str | None:
    """–£ ultralytics base validator –ø—Ä–∏ save_json —ñ –Ω–µ–ø–æ—Ä–æ–∂–Ω—å–æ–º—É jdict —Å–∞–º –ø–∏—à–µ save_dir/predictions.json. –¢—É—Ç –ª–∏—à–µ –¥–æ–±–∏—Ä–∞—î–º–æ: —è–∫—â–æ —Ñ–∞–π–ª—É –Ω–µ–º–∞—î (–Ω–∞–ø—Ä. jdict –ø–æ—Ä–æ–∂–Ω—ñ–π) ‚Äî –ø–∏—à–µ–º–æ –∑ jdict –∞–±–æ –∑–±–∏—Ä–∞—î–º–æ –∑ model.predict."""
    pred_path = os.path.join(save_dir, "predictions.json")
    if os.path.isfile(pred_path):
        return pred_path
    jdict = getattr(results, "jdict", None)
    if jdict and len(jdict) > 0:
        try:
            with open(pred_path, "w", encoding="utf-8") as f:
                json.dump(jdict, f, indent=2, ensure_ascii=False)
            return pred_path
        except Exception:
            pass
    model = getattr(results, "model", None)
    if not model or not img_paths:
        return None
    try:
        pred_results = model.predict(
            img_paths,
            imgsz=imgsz or VALIDATION_CONFIG.get("imgsz", 640),
            conf=VALIDATION_CONFIG.get("conf", 0.25),
            iou=VALIDATION_CONFIG.get("iou", 0.5),
            verbose=False,
            max_det=300,
        )
    except Exception:
        return None
    coco_preds = []
    for img_id, (img_path, result) in enumerate(zip(img_paths, pred_results)):
        if result.boxes is None or len(result.boxes) == 0:
            continue
        boxes = result.boxes
        xyxy = boxes.xyxy.cpu().numpy()
        confs = boxes.conf.cpu().numpy()
        clss = boxes.cls.cpu().numpy().astype(int)
        for i in range(len(boxes)):
            x1, y1, x2, y2 = xyxy[i].tolist()
            w = x2 - x1
            h = y2 - y1
            coco_preds.append({
                "image_id": img_id,
                "category_id": int(clss[i]) + 1,
                "bbox": [round(x1, 2), round(y1, 2), round(w, 2), round(h, 2)],
                "score": round(float(confs[i]), 4),
            })
    try:
        with open(pred_path, "w", encoding="utf-8") as f:
            json.dump(coco_preds, f, indent=2, ensure_ascii=False)
        return pred_path
    except Exception:
        return None


def _load_yolo_val_data(data_yaml_path: str, split: str = "val"):
    """–ü–æ–≤–µ—Ä—Ç–∞—î (list of image_paths, dataset_path) –∑ data.yaml –¥–ª—è YOLO. –Ø–∫—â–æ –ø–æ–º–∏–ª–∫–∞ ‚Äî ([], None)."""
    try:
        import yaml
        with open(data_yaml_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
    except Exception:
        return [], None
    path = data.get("path") or os.path.dirname(os.path.abspath(data_yaml_path))
    path = os.path.normpath(os.path.abspath(path))
    val_key = split if split in data else "val"
    val = data.get(val_key, "")
    if not val:
        return [], path
    val = os.path.normpath(val)
    if not os.path.isabs(val):
        val = os.path.join(path, val)
    if os.path.isfile(val) and val.endswith(".txt"):
        with open(val, "r", encoding="utf-8") as f:
            lines = [x.strip() for x in f if x.strip()]
        img_paths = [os.path.join(path, p) if not os.path.isabs(p) else p for p in lines]
    elif os.path.isdir(val):
        import glob as _g
        img_paths = []
        for ext in ("*.jpg", "*.jpeg", "*.png", "*.bmp"):
            img_paths.extend(_g.glob(os.path.join(val, ext)))
        img_paths.sort()
    else:
        img_paths = []
    return img_paths, path


def _yolo_label_path(img_path: str, dataset_path: str) -> str:
    """–®–ª—è—Ö –¥–æ .txt –ª–µ–π–±–ª–∞ YOLO –¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (images/ -> labels/, —ñ–Ω—à–∏–π —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è .txt)."""
    rel = os.path.relpath(img_path, dataset_path)
    rel = rel.replace("\\", "/")
    for old in ("images/", "Images/", "image/"):
        if old in rel:
            rel = rel.replace(old, "labels/", 1)
            break
    else:
        rel = "labels/" + os.path.basename(rel)
    base, _ = os.path.splitext(rel)
    return os.path.join(dataset_path, base + ".txt")


def _find_coco_annotations(data_yaml_path: str, split: str = "val") -> str | None:
    """–®—É–∫–∞—î —à–ª—è—Ö –¥–æ COCO annotations (instances_*.json) –∑ data.yaml."""
    try:
        import yaml
        with open(data_yaml_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
    except Exception:
        return None
    path = data.get("path") or os.path.dirname(os.path.abspath(data_yaml_path))
    path = os.path.normpath(path)
    ann_dir = os.path.join(path, "annotations")
    if not os.path.isdir(ann_dir):
        return None
    for name in ("instances_val2017.json", "instances_val.json", f"instances_{split}.json"):
        p = os.path.join(ann_dir, name)
        if os.path.isfile(p):
            return p
    import glob as _glob
    candidates = _glob.glob(os.path.join(ann_dir, "instances_*.json"))
    return candidates[0] if candidates else None


def _extract_per_class_ap_by_area(coco_eval) -> list[dict]:
    """
    –í–∏—Ç—è–≥—É—î –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É AP small / medium / large –∑ coco_eval.eval['precision'].
    COCO precision shape: (T, R, K, A, M) ‚Äî T=IoU, R=recall, K=classes, A=area (0:all,1:small,2:medium,3:large), M=maxDets.
    """
    import numpy as np
    out = []
    try:
        prec = coco_eval.eval.get("precision")
        if prec is None:
            return out
        prec = np.asarray(prec)
        # A: 0=all, 1=small, 2=medium, 3=large
        K = prec.shape[2]
        for k in range(K):
            ap_s = float(np.mean(prec[:, :, k, 1, :])) if prec.shape[3] > 1 else 0.0
            ap_m = float(np.mean(prec[:, :, k, 2, :])) if prec.shape[3] > 2 else 0.0
            ap_l = float(np.mean(prec[:, :, k, 3, :])) if prec.shape[3] > 3 else 0.0
            out.append({"small": ap_s, "medium": ap_m, "large": ap_l})
    except Exception:
        pass
    return out


def run_coco_eval_metrics(
    save_dir: str,
    data_yaml_path: str = None,
    split: str = None,
    annotation_path: str = None,
) -> dict:
    """
    –ó–∞–ø—É—Å–∫–∞—î COCO evaluation –∑–∞ predictions.json —Ç–∞ annotations, –ø–æ–≤–µ—Ä—Ç–∞—î –º–µ—Ç—Ä–∏–∫–∏.
    annotation_path: —è–∫—â–æ –∑–∞–¥–∞–Ω–æ ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —Ü–µ–π JSON (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ –∑ YOLO-–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó);
    —ñ–Ω–∞–∫—à–µ —à—É–∫–∞—é—Ç—å—Å—è COCO annotations —á–µ—Ä–µ–∑ _find_coco_annotations(data_yaml_path).
    """
    split = split or VALIDATION_CONFIG.get("split", "val")
    empty = {
        "ap_by_size": {"small": 0.0, "medium": 0.0, "large": 0.0},
        "ar_maxdets1": 0.0,
        "ar_maxdets10": 0.0,
        "ar_maxdets100": 0.0,
        "ar_small": 0.0,
        "ar_medium": 0.0,
        "ar_large": 0.0,
        "ap_by_class_area": [],
    }
    pred_json = os.path.join(save_dir, "predictions.json")
    if not os.path.isfile(pred_json):
        print(f"[COCO eval] –ù–µ–º–∞—î —Ñ–∞–π–ª—É: {pred_json}", flush=True)
        return empty
    anno_json = annotation_path if (annotation_path and os.path.isfile(annotation_path)) else None
    if not anno_json and data_yaml_path:
        anno_json = _find_coco_annotations(data_yaml_path, split)
    if not anno_json or not os.path.isfile(anno_json):
        print(f"[COCO eval] –ù–µ–º–∞—î annotations: {anno_json or annotation_path}", flush=True)
        return empty
    try:
        from pycocotools.coco import COCO
        from pycocotools.cocoeval import COCOeval
    except ImportError as e:
        print(f"[COCO eval] –ü–æ–º–∏–ª–∫–∞: {e}", flush=True)
        print("[COCO eval] –í—Å—Ç–∞–Ω–æ–≤–∏: pip install pycocotools  (AP by size, AR –±—É–¥—É—Ç—å 0 –ø–æ–∫–∏ –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ)", flush=True)
        return empty
    try:
        anno = COCO(anno_json)
        # –£ –ø—Ä–µ–¥–∏–∫—Ç—ñ–≤ ultralytics image_id = stem —Ñ–∞–π–ª—É; —É –Ω–∞—à–∏—Ö annotations ‚Äî 0,1,2,‚Ä¶. –ó–≤–æ–¥–∏–º–æ –¥–æ —Å–ø—ñ–ª—å–Ω–æ–≥–æ id –ø–æ file_name.
        with open(pred_json, "r", encoding="utf-8") as f:
            pred_list = json.load(f)
        images_list = anno.dataset.get("images", [])
        fn2id = {img["file_name"]: img["id"] for img in images_list}
        mapped = []
        for p in pred_list:
            fn = p.get("file_name")
            if fn and fn in fn2id:
                p = dict(p)
                p["image_id"] = fn2id[fn]
                mapped.append(p)
        if not mapped:
            print(f"[COCO eval] –ü—Ä–æ–ø—É—â–µ–Ω–æ: —É annotations {len(fn2id)} images, —É predictions {len(pred_list)} –∑–∞–ø–∏—Å—ñ–≤, –∑–±—ñ–≥—ñ–≤ –ø–æ file_name: 0. –ü–µ—Ä–µ–≤—ñ—Ä split.", flush=True)
            return empty
        pred = anno.loadRes(mapped)
        coco_eval = COCOeval(anno, pred, "bbox")
        img_ids = sorted(anno.getImgIds())
        if img_ids:
            coco_eval.params.imgIds = img_ids
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()
        # stats: 0=AP .5:.95, 1=AP .5, 2=AP .75, 3=AP small, 4=AP medium, 5=AP large,
        #        6=AR maxDets=1, 7=AR maxDets=10, 8=AR maxDets=100, 9=AR small, 10=AR medium, 11=AR large
        s = coco_eval.stats
        ap_by_class_area = _extract_per_class_ap_by_area(coco_eval)
        print(f"[COCO eval] OK: {len(fn2id)} images, {len(mapped)} predictions. AP small={s[3]:.4f}, medium={s[4]:.4f}, large={s[5]:.4f}", flush=True)
        return {
            "ap_by_size": {
                "small": float(s[3]) if len(s) > 3 else 0.0,
                "medium": float(s[4]) if len(s) > 4 else 0.0,
                "large": float(s[5]) if len(s) > 5 else 0.0,
            },
            "ar_maxdets1": float(s[6]) if len(s) > 6 else 0.0,
            "ar_maxdets10": float(s[7]) if len(s) > 7 else 0.0,
            "ar_maxdets100": float(s[8]) if len(s) > 8 else 0.0,
            "ar_small": float(s[9]) if len(s) > 9 else 0.0,
            "ar_medium": float(s[10]) if len(s) > 10 else 0.0,
            "ar_large": float(s[11]) if len(s) > 11 else 0.0,
            "ap_by_class_area": ap_by_class_area,
        }
    except Exception as e:
        print(f"[COCO eval] –ü–æ–º–∏–ª–∫–∞: {e}", flush=True)
        return empty


def _get_ap_by_size(validation_results: object) -> dict:
    """
    –°–ø—Ä–æ–±–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ AP small / medium / large –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó.
    –î–æ—Å—Ç—É–ø–Ω–æ –ª–∏—à–µ –ø—Ä–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó COCO/LVIS –¥–∞—Ç–∞—Å–µ—Ç—É (save_json=True, is_coco).
    """
    out = {"small": None, "medium": None, "large": None}
    try:
        # Ultralytics –∑–±–µ—Ä—ñ–≥–∞—î —Ü—ñ –º–µ—Ç—Ä–∏–∫–∏ –≤ stats –ø—ñ—Å–ª—è faster-coco-eval (COCO/LVIS)
        stats = getattr(validation_results, "get_stats", None)
        if callable(stats):
            s = stats()
            if isinstance(s, dict):
                out["small"] = s.get("metrics/mAP_small(B)")
                out["medium"] = s.get("metrics/mAP_medium(B)")
                out["large"] = s.get("metrics/mAP_large(B)")
        for k in list(out.keys()):
            if out[k] is not None:
                out[k] = _safe_float(out[k], None)
    except Exception:
        pass
    return out


def extract_metrics(validation_results: object) -> dict:
    """
    –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó.
    
    Args:
        validation_results: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –≤—ñ–¥ model.val()
    
    Returns:
        dict: –°–ª–æ–≤–Ω–∏–∫ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    box = validation_results.box
    precision = _safe_float(getattr(box, "mp", None))
    recall = _safe_float(getattr(box, "mr", None))
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

    metrics = {
        "mAP50": _safe_float(getattr(box, "map50", None)),
        "mAP50-95": _safe_float(getattr(box, "map", None)),
        "mAP75": _safe_float(getattr(box, "map75", None)),
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "ap_by_size": _get_ap_by_size(validation_results),
        "ar_maxdets1": 0.0,
        "ar_maxdets10": 0.0,
        "ar_maxdets100": 0.0,
        "ar_small": 0.0,
        "ar_medium": 0.0,
        "ar_large": 0.0,
        "ap_by_class_area": [],
    }

    # Per-class: mAP50, mAP50-95, precision, recall, F1 (—ñ–Ω–¥–µ–∫—Å –ø–æ ap_class_index)
    class_stats = {}
    ap_class_index = getattr(box, "ap_class_index", None)
    if hasattr(ap_class_index, "tolist"):
        ap_class_index = ap_class_index.tolist()
    elif not isinstance(ap_class_index, list):
        ap_class_index = list(range(len(CLASSES))) if ap_class_index is None else []

    maps = getattr(box, "maps", None)
    ap5095 = getattr(box, "ap", None)
    p_list = getattr(box, "p", None)
    r_list = getattr(box, "r", None)
    f1_list = getattr(box, "f1", None)

    for class_id, _ in CLASSES.items():
        try:
            idx = ap_class_index.index(int(class_id)) if ap_class_index else class_id
        except (ValueError, AttributeError):
            idx = int(class_id)
        m50 = None
        if maps is not None:
            if class_id < len(maps):
                m50 = maps[class_id]
            elif idx < len(maps):
                m50 = maps[idx]
        stat = {
            "mAP50": _safe_float(m50),
            "mAP50-95": _safe_float(ap5095[idx] if ap5095 is not None and idx < len(ap5095) else None),
            "precision": _safe_float(p_list[idx] if p_list is not None and idx < len(p_list) else None),
            "recall": _safe_float(r_list[idx] if r_list is not None and idx < len(r_list) else None),
            "f1": _safe_float(f1_list[idx] if f1_list is not None and idx < len(f1_list) else None),
        }
        if stat["f1"] == 0.0 and stat["precision"] and stat["recall"]:
            p, r = stat["precision"], stat["recall"]
            stat["f1"] = 2 * p * r / (p + r) if (p + r) > 0 else 0.0
        class_stats[str(class_id)] = stat

    metrics["class_stats"] = class_stats
    return metrics


def get_speed_info(validation_results: object) -> dict:
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ —à–≤–∏–¥–∫—ñ—Å—Ç—å."""
    speed_info = {}
    if hasattr(validation_results, "speed"):
        speed = validation_results.speed
        speed_info = {
            "preprocess_ms": speed.get("preprocess", 0),
            "inference_ms": speed.get("inference", 0),
            "postprocess_ms": speed.get("postprocess", 0),
        }
        total_time = sum(speed_info.values())
        speed_info["total_ms"] = total_time
        speed_info["fps"] = round(1000 / total_time, 2) if total_time > 0 else 0
    return speed_info


def save_results_json(
    metrics: dict,
    speed_info: dict,
    model_path: str,
    output_dir: str
) -> str:
    """
    –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É —î–¥–∏–Ω–∏–π JSON —Ñ–∞–π–ª.
    
    Args:
        metrics: –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        speed_info: –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —à–≤–∏–¥–∫—ñ—Å—Ç—å
        model_path: –®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    
    Returns:
        str: –®–ª—è—Ö –¥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ–≥–æ —Ñ–∞–π–ª—É
    """
    ap_size = metrics.get("ap_by_size") or {}
    results = {
        "metrics": {
            "mAP50": metrics["mAP50"],
            "mAP50-95": metrics["mAP50-95"],
            "mAP75": metrics.get("mAP75"),
            "precision": metrics["precision"],
            "recall": metrics["recall"],
            "f1": metrics["f1"],
            "ap_small": ap_size.get("small"),
            "ap_medium": ap_size.get("medium"),
            "ap_large": ap_size.get("large"),
            "ar_maxdets1": metrics.get("ar_maxdets1"),
            "ar_maxdets10": metrics.get("ar_maxdets10"),
            "ar_maxdets100": metrics.get("ar_maxdets100"),
            "ar_small": metrics.get("ar_small"),
            "ar_medium": metrics.get("ar_medium"),
            "ar_large": metrics.get("ar_large"),
            "class_stats": metrics.get("class_stats", {}),
            "ap_by_class_area": metrics.get("ap_by_class_area", []),
        },
        "num_classes": len(CLASSES),
        "classes": list(CLASSES.values()),
        "inference_fps": speed_info.get("fps", 0),
        "inference_latency_ms": speed_info.get("total_ms", 0),
        "split": VALIDATION_CONFIG["split"],
        "dataset_dir": DATASET_ROOT,
        "validation_date": datetime.now().isoformat(),
        "inference_config": {
            "conf_threshold": VALIDATION_CONFIG["conf"],
            "iou_threshold": VALIDATION_CONFIG["iou"],
            "max_det": VALIDATION_CONFIG["max_det"],
            "classes": VALIDATION_CONFIG["classes"],
            "agnostic_nms": VALIDATION_CONFIG["agnostic_nms"],
            "half": VALIDATION_CONFIG["half"],
            "batch_size": VALIDATION_CONFIG["batch"],
            "imgsz": VALIDATION_CONFIG["imgsz"],
            "workers": VALIDATION_CONFIG["workers"],
            "device": VALIDATION_CONFIG["device"] or "cuda",
        },
        "model_path": model_path,
    }
    
    json_path = os.path.join(output_dir, "validation_results.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    return json_path


def _plot_per_class_metrics(metrics: dict, output_dir: str) -> str | None:
    """
    –ë—É–¥—É—î bar chart mAP50 —Ç–∞ mAP50-95 –ø–æ –∫–ª–∞—Å–∞—Ö, –∑–±–µ—Ä—ñ–≥–∞—î –≤ output_dir.
    –ü–æ–≤–µ—Ä—Ç–∞—î —ñ–º'—è —Ñ–∞–π–ª—É –∞–±–æ None —è–∫—â–æ –ø–æ–±—É–¥–æ–≤–∞ –Ω–µ–º–æ–∂–ª–∏–≤–∞.
    """
    if not _HAS_MATPLOTLIB:
        return None
    class_stats = metrics.get("class_stats", {})
    if not class_stats:
        return None
    names = [CLASSES[i] for i in range(len(CLASSES))]
    m50 = [class_stats.get(str(i), {}).get("mAP50", 0) for i in range(len(CLASSES))]
    m5095 = [class_stats.get(str(i), {}).get("mAP50-95", 0) for i in range(len(CLASSES))]
    x = np.arange(len(names))
    w = 0.35
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.bar(x - w / 2, m50, w, label="mAP@0.5", color="steelblue")
    ax.bar(x + w / 2, m5095, w, label="mAP@0.5:0.95", color="coral", alpha=0.9)
    ax.set_xticks(x)
    ax.set_xticklabels(names)
    ax.set_ylabel("Score")
    ax.set_title("Per-class mAP")
    ax.legend()
    ax.set_ylim(0, 1.05)
    fig.tight_layout()
    path = os.path.join(output_dir, "per_class_map.png")
    fig.savefig(path, dpi=120, bbox_inches="tight")
    plt.close(fig)
    return "per_class_map.png"


def generate_markdown_report(
    metrics: dict,
    speed_info: dict,
    model_path: str,
    output_dir: str
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ markdown –∑–≤—ñ—Ç—É.
    
    Args:
        metrics: –°–ª–æ–≤–Ω–∏–∫ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        speed_info: –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —à–≤–∏–¥–∫—ñ—Å—Ç—å
        model_path: –®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
    
    Returns:
        str: –®–ª—è—Ö –¥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ–≥–æ –∑–≤—ñ—Ç—É
    """
    model_name = Path(model_path).stem
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    per_class_plot = _plot_per_class_metrics(metrics, output_dir)

    # –§–æ—Ä–º—É—î–º–æ –∑–≤—ñ—Ç —É —Ñ–æ—Ä–º–∞—Ç—ñ —è–∫ —É –ø—Ä–∏–∫–ª–∞–¥—ñ
    report_content = f"""# üéØ YOLO Validation Report

## Experiment Overview

| **Parameter** | **Value** |
|---------------|-----------|
| **Model** | `{model_name}` |
| **Model Path** | `{model_path}` |
| **Date & Time** | {current_time} |
| **Dataset** | `{DATASET_ROOT}` |
| **Split** | `{VALIDATION_CONFIG['split']}` |
| **Object Categories** | {len(CLASSES)} |

## Configuration Settings

| **Setting** | **Value** |
|-------------|-----------|
| **Confidence Threshold** | {VALIDATION_CONFIG['conf']} |
| **IoU Threshold** | {VALIDATION_CONFIG['iou']} |
| **Image Size** | {VALIDATION_CONFIG['imgsz']} |
| **Batch Size** | {VALIDATION_CONFIG['batch']} |
| **Half (FP16)** | {VALIDATION_CONFIG['half']} |
| **Device** | {VALIDATION_CONFIG['device'] or 'cuda'} |

---

## üìä Overall Performance

| **Metric** | **Value** |
|------------|-----------|
| **mAP@0.5** | {metrics['mAP50']:.4f} |
| **mAP@0.5:0.95** | {metrics['mAP50-95']:.4f} |
| **mAP@0.75** | {metrics.get('mAP75', 0):.4f} |
| **Precision** | {metrics['precision']:.4f} |
| **Recall** | {metrics['recall']:.4f} |
| **F1 Score** | {metrics['f1']:.4f} |

---
"""
    # AP –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º –æ–±'—î–∫—Ç—ñ–≤ (small / medium / large) ‚Äî –∑–∞–≥–∞–ª–æ–º
    ap_size = metrics.get("ap_by_size") or {}
    ap_s, ap_m, ap_l = ap_size.get("small"), ap_size.get("medium"), ap_size.get("large")
    def _fmt_ap(v):
        return f"{v:.4f}" if v is not None else "‚Äî"
    report_content += """
## üìê AP –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º –æ–±'—î–∫—Ç—ñ–≤ (overall)

| **–†–æ–∑–º—ñ—Ä** | **AP@0.5:0.95** | **–ü—Ä–∏–º—ñ—Ç–∫–∞** |
|------------|-----------------|--------------|
| **Small** (area < 32¬≤ px) | """ + _fmt_ap(ap_s) + """ | –ú–∞–ª–µ–Ω—å–∫—ñ –æ–±'—î–∫—Ç–∏ |
| **Medium** (32¬≤‚Äì96¬≤ px) | """ + _fmt_ap(ap_m) + """ | –°–µ—Ä–µ–¥–Ω—ñ –æ–±'—î–∫—Ç–∏ |
| **Large** (area ‚â• 96¬≤ px) | """ + _fmt_ap(ap_l) + """ | –í–µ–ª–∏–∫—ñ –æ–±'—î–∫—Ç–∏ |

"""
    if ap_s is None and ap_m is None and ap_l is None:
        report_content += "*AP –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º –¥–æ—Å—Ç—É–ø–Ω—ñ –ø—Ä–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –¥–∞—Ç–∞—Å–µ—Ç—É —É —Ñ–æ—Ä–º–∞—Ç—ñ COCO (save_json=True, annotations —É COCO).*\n\n---\n\n"
    else:
        report_content += "---\n\n"

    # AR (Average Recall) ‚Äî –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏ COCO
    ar1 = metrics.get("ar_maxdets1", 0)
    ar10 = metrics.get("ar_maxdets10", 0)
    ar100 = metrics.get("ar_maxdets100", 0)
    ar_s = metrics.get("ar_small", 0)
    ar_m = metrics.get("ar_medium", 0)
    ar_l = metrics.get("ar_large", 0)
    report_content += """
### –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏ (COCO): AR ‚Äî Average Recall

| **–ú–µ—Ç—Ä–∏–∫–∞** | **–ó–Ω–∞—á–µ–Ω–Ω—è** |
|-------------|--------------|
| **AR @ maxDets=1** | """ + f"{ar1:.4f}" + """ |
| **AR @ maxDets=10** | """ + f"{ar10:.4f}" + """ |
| **AR @ maxDets=100** | """ + f"{ar100:.4f}" + """ |
| **AR small** | """ + f"{ar_s:.4f}" + """ |
| **AR medium** | """ + f"{ar_m:.4f}" + """ |
| **AR large** | """ + f"{ar_l:.4f}" + """ |

---

"""
    # AP –ø–æ –∫–ª–∞—Å–∞—Ö –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º –æ–±'—î–∫—Ç–∞
    ap_by_class_area = metrics.get("ap_by_class_area") or []
    if ap_by_class_area:
        report_content += """## AP –ø–æ –∫–ª–∞—Å–∞—Ö –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º –æ–±'—î–∫—Ç–∞

| **Class** | **AP small** | **AP medium** | **AP large** |
|-----------|---------------|---------------|--------------|
"""
        for i, class_name in CLASSES.items():
            row = ap_by_class_area[i] if i < len(ap_by_class_area) else {}
            report_content += "| {} | {:.4f} | {:.4f} | {:.4f} |\n".format(
                class_name,
                row.get("small", 0),
                row.get("medium", 0),
                row.get("large", 0),
            )
        report_content += "\n---\n\n"

    report_content += """## üìã Per-Class Performance

| **Class** | **mAP@0.5** | **mAP@0.5:0.95** | **Precision** | **Recall** | **F1** |
|-----------|-------------|------------------|---------------|-----------|--------|
"""
    class_stats = metrics.get("class_stats", {})
    for class_id, class_name in CLASSES.items():
        stat = class_stats.get(str(class_id), {})
        m50 = stat.get("mAP50", 0)
        m5095 = stat.get("mAP50-95", 0)
        prec = stat.get("precision", 0)
        rec = stat.get("recall", 0)
        f1 = stat.get("f1", 0)
        report_content += f"| {class_name} | {m50:.4f} | {m5095:.4f} | {prec:.4f} | {rec:.4f} | {f1:.4f} |\n"

    report_content += """
### –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è per-class –º–µ—Ç—Ä–∏–∫

- **mAP@0.5** ‚Äî —Å–µ—Ä–µ–¥–Ω—è —Ç–æ—á–Ω—ñ—Å—Ç—å –ø—Ä–∏ IoU 0.5 (–º‚Äô—è–∫—à–∏–π –∫—Ä–∏—Ç–µ—Ä—ñ–π).
- **mAP@0.5:0.95** ‚Äî —Å–µ—Ä–µ–¥–Ω—è —Ç–æ—á–Ω—ñ—Å—Ç—å –ø–æ IoU 0.5‚Äì0.95 (–∂–æ—Ä—Å—Ç–∫—ñ—à–∏–π, COCO-style).
- **Precision** ‚Äî —á–∞—Å—Ç–∫–∞ –∫–æ—Ä–µ–∫—Ç–Ω–∏—Ö –¥–µ—Ç–µ–∫—Ü—ñ–π —Å–µ—Ä–µ–¥ —É—Å—ñ—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å –∫–ª–∞—Å—É.
- **Recall** ‚Äî —á–∞—Å—Ç–∫–∞ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –æ–±‚Äô—î–∫—Ç—ñ–≤ –∫–ª–∞—Å—É —Å–µ—Ä–µ–¥ —É—Å—ñ—Ö GT.
- **F1** ‚Äî –±–∞–ª–∞–Ω—Å –º—ñ–∂ precision —Ç–∞ recall.

---

## ‚ö° Inference Speed

| **Metric** | **Value** |
|------------|-----------|
| **FPS** | """ + f"{speed_info.get('fps', 0):.1f}" + """ |
| **Latency** | """ + f"{speed_info.get('total_ms', 0):.2f}" + """ ms/image |
| **Preprocess** | """ + f"{speed_info.get('preprocess_ms', 0):.2f}" + """ ms |
| **Inference** | """ + f"{speed_info.get('inference_ms', 0):.2f}" + """ ms |
| **Postprocess** | """ + f"{speed_info.get('postprocess_ms', 0):.2f}" + """ ms |

---

## üìà –ì—Ä–∞—Ñ—ñ–∫–∏ —Ç–∞ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏

"""
    if per_class_plot:
        report_content += f"**Per-class mAP:**\n\n![Per-class mAP]({per_class_plot})\n\n"
    # –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –≥—Ä–∞—Ñ—ñ–∫–∏, –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ Ultralytics (plots=True); detect task —á–∞—Å—Ç–æ –∑ –ø—Ä–µ—Ñ—ñ–∫—Å–æ–º Box
    plot_names = [
        ("confusion_matrix.png", "Confusion Matrix"),
        ("BoxPR_curve.png", "Precision-Recall curve"),
        ("BoxF1_curve.png", "F1 Score vs Confidence"),
        ("BoxP_curve.png", "Precision vs Confidence"),
        ("BoxR_curve.png", "Recall vs Confidence"),
        ("PR_curve.png", "PR curve (alt)"),
        ("F1_curve.png", "F1 curve (alt)"),
        ("confusion_matrix_normalized.png", "Confusion Matrix (normalized)"),
    ]
    report_content += "### –ì—Ä–∞—Ñ—ñ–∫–∏ —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó\n\n–ì—Ä–∞—Ñ—ñ–∫–∏ –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó (—è–∫—â–æ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤ —Ü—ñ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó):\n\n"
    for fname, desc in plot_names:
        report_content += f"- **{desc}**: [{fname}]({fname})\n"
    report_content += """

### –ö–æ—Ä–∏—Å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É

| –ú–µ—Ç—Ä–∏–∫–∞ | –ù–∞–≤—ñ—â–æ |
|---------|--------|
| **mAP@0.5** | –ó–∞–≥–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å –ø—Ä–∏ —Ç–∏–ø–æ–≤–æ–º—É IoU. |
| **mAP@0.75** | –°—Ç—Ä–æ–≥—ñ—à–∞ –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—è –±–æ–∫—Å—ñ–≤. |
| **mAP@0.5:0.95** | –ó–≤–µ–¥–µ–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ (COCO standard). |
| **Precision** | –í–∞–∂–ª–∏–≤–∞, —è–∫—â–æ –∫—Ä–∏—Ç–∏—á–Ω—ñ false positives. |
| **Recall** | –í–∞–∂–ª–∏–≤–∞, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞—Ç–∏ –æ–±'—î–∫—Ç–∏. |
| **F1** | –ë–∞–ª–∞–Ω—Å –º—ñ–∂ precision —ñ recall. |
| **AP small/medium/large** | –Ø–∫—ñ—Å—Ç—å –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö –æ–±'—î–∫—Ç—ñ–≤ (COCO). |

---

*üìä Report generated by YOLO Validation System*  
*üïê """ + current_time + """*
"""
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–≤—ñ—Ç
    report_path = os.path.join(output_dir, "validation_report.md")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_content)
    
    return report_path


def print_summary(metrics: dict, speed_info: dict, json_path: str, report_path: str) -> None:
    """–í–∏–≤–µ–¥–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫—É —É –∫–æ–Ω—Å–æ–ª—å."""
    print()
    print(f"üìä Inference Speed: {speed_info.get('fps', 0):.1f} FPS ({speed_info.get('total_ms', 0):.2f} ms/image)")
    print()
    print("=" * 50)
    print("YOLO VALIDATION SUMMARY")
    print("=" * 50)
    print(f"mAP@0.5:      {metrics['mAP50']:.4f}")
    print(f"mAP@0.5:0.95: {metrics['mAP50-95']:.4f}")
    if metrics.get("mAP75") is not None:
        print(f"mAP@0.75:     {metrics['mAP75']:.4f}")
    print(f"Precision:    {metrics['precision']:.4f}")
    print(f"Recall:       {metrics['recall']:.4f}")
    print(f"F1 Score:     {metrics['f1']:.4f}")
    ap_size = metrics.get("ap_by_size") or {}
    print("AP by size:   small={}, medium={}, large={}".format(
        f"{ap_size.get('small') or 0:.4f}" if ap_size.get("small") is not None else "‚Äî",
        f"{ap_size.get('medium') or 0:.4f}" if ap_size.get("medium") is not None else "‚Äî",
        f"{ap_size.get('large') or 0:.4f}" if ap_size.get("large") is not None else "‚Äî",
    ))
    print("AR:           maxDets1={:.4f}, maxDets10={:.4f}, maxDets100={:.4f}".format(
        metrics.get("ar_maxdets1") or 0, metrics.get("ar_maxdets10") or 0, metrics.get("ar_maxdets100") or 0
    ))
    print("AR by size:   small={:.4f}, medium={:.4f}, large={:.4f}".format(
        metrics.get("ar_small") or 0, metrics.get("ar_medium") or 0, metrics.get("ar_large") or 0
    ))
    print(f"JSON Results: {json_path}")
    print(f"MD Report:    {report_path}")
    print("=" * 50)
    print()
    print(f"[Results] –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {json_path}")


def save_results(
    validation_results: object,
    metrics: dict,
    model_path: str = TRAINED_MODEL_PATH,
    output_dir: str = None
) -> dict:
    """
    –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó.
    
    Args:
        validation_results: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        metrics: –í–∏—Ç—è–≥–Ω—É—Ç—ñ –º–µ—Ç—Ä–∏–∫–∏
        model_path: –®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ (–¥–ª—è –∑–≤—ñ—Ç—É)
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    
    Returns:
        dict: –°–ª–æ–≤–Ω–∏–∫ –∑—ñ —à–ª—è—Ö–∞–º–∏ –¥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
    """
    if output_dir is None:
        output_dir = os.path.join(PROJECT_DIR, EXPERIMENT_NAME)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # –û—Ç—Ä–∏–º—É—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —à–≤–∏–¥–∫—ñ—Å—Ç—å
    speed_info = get_speed_info(validation_results)
    
    saved_files = {}
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É —î–¥–∏–Ω–∏–π JSON —Ñ–∞–π–ª
    json_path = save_results_json(metrics, speed_info, model_path, output_dir)
    saved_files["validation_json"] = json_path
    
    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è markdown –∑–≤—ñ—Ç—É
    report_path = generate_markdown_report(metrics, speed_info, model_path, output_dir)
    saved_files["markdown_report"] = report_path
    
    # –í–∏–≤–µ–¥–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫—É
    print_summary(metrics, speed_info, json_path, report_path)
    
    return saved_files


def main(
    model_path: str = TRAINED_MODEL_PATH,
    save_results_flag: bool = True,
    **kwargs
):
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–ø—É—Å–∫—É –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó.
    
    Args:
        model_path: –®–ª—è—Ö –¥–æ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
        save_results_flag: –ß–∏ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É —Ñ–∞–π–ª–∏
        **kwargs: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
    """
    # –ù–µ —Å—Ç–≤–æ—Ä—é—î–º–æ –ø–∞–ø–∫—É —ñ –Ω–µ –ø–µ—Ä–µ–¥–∞—î–º–æ save_dir: ultralytics —Å–∞–º —Å—Ç–≤–æ—Ä–∏—Ç—å –æ–¥–Ω—É –ø–∞–ø–∫—É –∑–∞ –∑–∞–ø—É—Å–∫ (val, val2, val3‚Ä¶).
    # –†–∞–Ω—ñ—à–µ –º–∏ —Ä–æ–±–∏–ª–∏ makedirs –¥–æ val —ñ –ø–µ—Ä–µ–¥–∞–≤–∞–ª–∏ save_dir ‚Äî –≤–∏—Ö–æ–¥–∏–ª–æ –¥–≤—ñ –ø–∞–ø–∫–∏: –æ–¥–Ω–∞ –ø–æ—Ä–æ–∂–Ω—è, –æ–¥–Ω–∞ –∑–∞–ø–æ–≤–Ω–µ–Ω–∞.
    save_dir = os.path.join(PROJECT_DIR, EXPERIMENT_NAME)
    split = VALIDATION_CONFIG.get("split", "val")
    imgsz = VALIDATION_CONFIG.get("imgsz", 640)
    img_paths, _ = _load_yolo_val_data(YAML_PATH, split)

    results = validate_model(model_path=model_path, **kwargs)
    save_dir = str(getattr(results, "save_dir", None) or save_dir)

    # GT –∑ .txt ‚Üí COCO JSON (pycocotools –±–µ–∑ —Ü—å–æ–≥–æ –Ω–µ –ø—Ä–∞—Ü—é—î)
    anno_val_path = os.path.join(save_dir, "annotations_val.json")
    yolo_split_to_coco_annotations(YAML_PATH, anno_val_path, split=split, imgsz=imgsz)
    # predictions.json –≤–∞–ª—ñ–¥–∞—Ç–æ—Ä –≤–∂–µ –ø–∏—à–µ –ø—Ä–∏ save_json (engine/validator.py); –¥–æ–±–∏—Ä–∞—î–º–æ –ª–∏—à–µ —è–∫—â–æ —Ñ–∞–π–ª—É –Ω–µ–º–∞—î
    _ensure_predictions_coco_json(results, save_dir, img_paths, imgsz)

    metrics = extract_metrics(results)
    coco_metrics = run_coco_eval_metrics(save_dir, YAML_PATH, annotation_path=anno_val_path)
    defaults = {"ap_by_size": metrics.get("ap_by_size"), "ap_by_class_area": []}
    for k in ("ap_by_size", "ar_maxdets1", "ar_maxdets10", "ar_maxdets100", "ar_small", "ar_medium", "ar_large", "ap_by_class_area"):
        metrics[k] = coco_metrics.get(k, defaults.get(k, 0.0))

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ (—É —Ç—É –∂ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é, —â–æ –π –≤–∞–ª—ñ–¥–∞—Ç–æ—Ä)
    if save_results_flag:
        save_results(results, metrics, model_path=model_path, output_dir=save_dir)
    
    return results, metrics


if __name__ == "__main__":
    main()
